import torch

import backgammon_env
import model
import network
import tes


def roll():
    return torch.randint(1, 7, (1,))[0].item()


def test_predictability_of_network():
    random_seed = 42

    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    torch.manual_seed(random_seed)

    nn = network.layered(198, 40, 4)

    b = backgammon_env.Backgammon(roll=roll)
    s0 = b.s0()
    tensor = tes.observe(s0)
    v = nn(tensor).tolist()
    expected = [
        0.3743259906768799,
        0.5034601092338562,
        0.4764396548271179,
        0.43971505761146545,
    ]
    assert v == expected


def test_predictability_of_network_after_train():
    random_seed = 42

    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    torch.manual_seed(random_seed)

    nn = network.layered(198, 40, 4)

    b = backgammon_env.Backgammon(roll=roll)
    s = b.s0()
    m = ((8, 5), (6, 5))
    s1 = b.next(s, m)
    observe = tes.observe
    trainer = model.Trainer(b, nn, observe)

    v = nn(observe(s))

    v1 = nn(observe(s1))

    assert v.tolist() == [
        0.3743259906768799,
        0.5034601092338562,
        0.4764396548271179,
        0.43971505761146545,
    ]
    assert v1.tolist() == [
        0.37185654044151306,
        0.5025563836097717,
        0.47492071986198425,
        0.43810516595840454,
    ]

    trainer.train(v1.dot(network.utility_tensor()).item(), s)

    assert nn(observe(s)).tolist() == [
        0.37418854236602783,
        0.5033813714981079,
        0.47652003169059753,
        0.43986862897872925,
    ]
    assert nn(observe(s1)).tolist() == [
        0.3717195987701416,
        0.5024778246879578,
        0.4750007390975952,
        0.43825823068618774,
    ]


def test_evaluations_of_single_game_play():
    random_seed = 42
    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    torch.manual_seed(random_seed)
    nn = network.layered(198, 20, 4)
    bck = backgammon_env.Backgammon(roll=roll)
    trainer = model.Trainer(bck, nn, tes.observe)

    for i in range(5):
        trainer.td_episode(i)

    s = bck.s0()
    evaluations = []

    rolls = []
    while True:
        (board, player_1, dice) = s
        evaluations.append(trainer.v(s).item())
        if bck.done(s):
            break
        else:
            rolls.append(dice)
            move = trainer.best(s)
            s = bck.next(s, move)

    assert rolls == [
        (3, 6),
        (4, 4),
        (4, 4),
        (6, 1),
        (4, 6),
        (4, 5),
        (2, 1),
        (6, 4),
        (4, 6),
        (6, 4),
        (4, 1),
        (4, 2),
        (6, 6),
        (5, 5),
        (5, 3),
        (2, 1),
        (3, 5),
        (4, 4),
        (1, 3),
        (3, 5),
        (6, 4),
        (1, 2),
        (3, 1),
        (4, 4),
        (3, 1),
        (6, 2),
        (6, 5),
        (3, 4),
        (1, 1),
        (5, 5),
        (1, 3),
        (6, 5),
        (1, 1),
        (3, 1),
        (6, 4),
        (3, 6),
        (5, 1),
        (1, 1),
        (6, 1),
        (1, 1),
        (1, 4),
        (5, 1),
        (1, 1),
        (6, 5),
        (4, 3),
        (1, 3),
        (5, 1),
        (1, 4),
        (1, 6),
        (1, 3),
        (5, 1),
        (5, 3),
        (2, 6),
        (5, 2),
        (2, 4),
        (6, 6),
        (4, 6),
        (2, 2),
        (6, 4),
        (3, 5),
        (4, 4),
        (1, 1),
        (5, 3),
        (5, 1),
        (5, 3),
        (3, 5),
        (3, 2),
        (4, 4),
        (2, 4),
        (2, 3),
        (1, 4),
        (5, 2),
        (5, 4),
        (6, 6),
        (4, 5),
        (3, 5),
        (3, 6),
        (2, 4),
        (5, 3),
        (5, 6),
        (2, 3),
        (2, 5),
        (4, 2),
        (3, 1),
        (5, 2),
        (4, 1),
        (4, 4),
        (1, 5),
        (2, 2),
        (2, 2),
        (2, 5),
        (5, 3),
        (5, 1),
        (4, 6),
        (6, 5),
        (2, 1),
        (4, 6),
        (6, 5),
        (1, 4),
        (1, 6),
        (2, 4),
        (2, 3),
        (2, 4),
        (5, 4),
        (1, 2),
        (2, 1),
        (6, 3),
        (5, 2),
        (5, 6),
        (6, 2),
        (6, 2),
        (2, 4),
        (4, 1),
        (5, 4),
        (6, 5),
        (4, 2),
        (1, 3),
        (2, 6),
        (1, 1),
        (3, 4),
        (1, 5),
        (5, 1),
        (3, 4),
        (6, 2),
        (6, 3),
        (4, 5),
        (2, 5),
        (3, 1),
        (1, 6),
        (1, 3),
        (2, 1),
        (1, 4),
        (3, 2),
        (6, 4),
        (3, 2),
        (1, 3),
        (2, 6),
        (6, 3),
        (3, 3),
        (5, 6),
        (3, 4),
        (2, 4),
        (4, 6),
        (6, 5),
        (1, 6),
        (5, 5),
        (3, 2),
        (2, 6),
        (5, 2),
        (6, 1),
        (3, 1),
        (2, 5),
        (2, 3),
        (2, 2),
        (4, 5),
        (4, 5),
        (1, 5),
        (1, 2),
        (2, 5),
        (2, 4),
        (2, 5),
        (4, 1),
        (2, 4),
        (3, 2),
        (4, 3),
        (2, 1),
        (4, 6),
        (4, 6),
    ]

    assert evaluations == [
        0.9273859262466431,
        0.9108179807662964,
        0.9328393936157227,
        0.91138756275177,
        0.9155396223068237,
        0.9072080850601196,
        0.9107073545455933,
        0.9017766714096069,
        0.9034852981567383,
        0.8990631103515625,
        0.9007046222686768,
        0.8937664031982422,
        0.8954540491104126,
        0.9002103805541992,
        0.9019161462783813,
        0.9046415090560913,
        0.9100974798202515,
        0.9121456146240234,
        0.913774847984314,
        0.9154808521270752,
        0.9112989902496338,
        0.9104353189468384,
        0.9169414043426514,
        0.9152132272720337,
        0.932844877243042,
        0.9477407932281494,
        0.9570443630218506,
        0.963908314704895,
        0.9673857688903809,
        0.9453120231628418,
        0.9288374185562134,
        0.9311256408691406,
        0.9225590229034424,
        0.9024549722671509,
        0.9018518924713135,
        0.8880007266998291,
        0.9014685153961182,
        0.8970003128051758,
        0.8986916542053223,
        0.8827084302902222,
        0.8844006061553955,
        0.8799099922180176,
        0.8817157745361328,
        0.8674640655517578,
        0.8759853839874268,
        0.8905940055847168,
        0.8941913843154907,
        0.8992519378662109,
        0.900985836982727,
        0.8932430744171143,
        0.9050987958908081,
        0.8859935998916626,
        0.8907307386398315,
        0.8802464008331299,
        0.8881348371505737,
        0.8771518468856812,
        0.8988335132598877,
        0.8969709873199463,
        0.9044601917266846,
        0.9025903940200806,
        0.9067769050598145,
        0.9049732685089111,
        0.910747766494751,
        0.917250394821167,
        0.9131319522857666,
        0.9181574583053589,
        0.9222893714904785,
        0.9071307182312012,
        0.9087808132171631,
        0.913235068321228,
        0.9217057228088379,
        0.9062498807907104,
        0.9070605039596558,
        0.8894380331039429,
        0.8967509269714355,
        0.9040148258209229,
        0.9046646356582642,
        0.9112858772277832,
        0.9129366874694824,
        0.9025685787200928,
        0.914036750793457,
        0.8919572830200195,
        0.9000039100646973,
        0.9111084938049316,
        0.9237567186355591,
        0.9394592046737671,
        0.9410117864608765,
        0.9394246339797974,
        0.941569447517395,
        0.9400153160095215,
        0.9470995664596558,
        0.9455034732818604,
        0.9399803876876831,
        0.9433648586273193,
        0.9514596462249756,
        0.9392896890640259,
        0.938493013381958,
        0.9307495355606079,
        0.9341130256652832,
        0.9317631721496582,
        0.9287488460540771,
        0.915861964225769,
        0.9275832176208496,
        0.9298410415649414,
        0.9215773344039917,
        0.921454668045044,
        0.9229929447174072,
        0.9121659994125366,
        0.9198780059814453,
        0.9109964370727539,
        0.9220528602600098,
        0.9118900299072266,
        0.9231665134429932,
        0.9160730838775635,
        0.9277940988540649,
        0.9273662567138672,
        0.9248777627944946,
        0.9154635667800903,
        0.909250020980835,
        0.915407657623291,
        0.9130257368087769,
        0.8886723518371582,
        0.8960216045379639,
        0.9060655832290649,
        0.9103772640228271,
        0.899594783782959,
        0.8870856761932373,
        0.8947606086730957,
        0.8968337774276733,
        0.8956512212753296,
        0.9186050891876221,
        0.9132765531539917,
        0.9181139469146729,
        0.8966164588928223,
        0.9121425151824951,
        0.9065464735031128,
        0.9136039018630981,
        0.9047039747238159,
        0.9115755558013916,
        0.9099699258804321,
        0.9068589210510254,
        0.905269980430603,
        0.8965247869491577,
        0.8953230381011963,
        0.9019894599914551,
        0.9009096622467041,
        0.8991560935974121,
        0.8966137170791626,
        0.8867009878158569,
        0.8779873847961426,
        0.8943443298339844,
        0.898932933807373,
        0.8973897695541382,
        0.9070249795913696,
        0.9085676670074463,
        0.897781252861023,
        0.9205355644226074,
        0.9228167533874512,
        0.9166598320007324,
        0.9104269742965698,
        0.9088702201843262,
        0.9089388847351074,
        0.9055514335632324,
        0.9059641361236572,
        0.9193239212036133,
        0.9167889356613159,
        0.9185197353363037,
        0.9108155965805054,
        0.9143836498260498,
    ]
